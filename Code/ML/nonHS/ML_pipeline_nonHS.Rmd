---
title: "ML_Trial"
author: "Andrea,Gloria,Lorenzo,Thomas"
date: "`r Sys.Date()`"
output: html_document
---
# Preparatory steps
  Loading packages
  Store package names in a vectors for ease of access and to load them easily 
```{r Setup, message=FALSE, warning=FALSE}
# Define a list of packages to be loaded
PACKAGES <- c(
  "caret",           # For machine learning model training and evaluation
  "class",           # Various classification methods
  "dplyr",           # To manage data frames more efficiently
  "e1071",           # For SVM modeling and much MORE!
  "glmnet",          # For the LASSO regression 
  "kknn",            # For k-nearest neighbors classification
  "kernlab",         # Kernel-based machine learning methods
  "randomForest",    # For random forest modeling
  "RRF",             # Regularized Random Forest
  "xgboost",         # For the XGBoost (Extreme Gradient Boosting) method
  "corrplot",        # For correlation plot visualization
  "factoextra",      # Additional tools for clustering and factor analysis
  "FactoMineR",      # For exploratory data analysis and multivariate analysis
  "ggplot2",         # For data visualization
  "plotly",          # Interactive plots
  "RColorBrewer",    # Color palettes for better visualizations
  "tidyverse",       # Collection of packages for data manipulation and visualization
  "plyr",            # Tools for data manipulation
  "MASS",            # Modern Applied Statistics with S
  "HiClimR",         # For hierarchical clustering
  "umap",            # Uniform Manifold Approximation and Projection for dimensional reduction
  "nnet"            # Simple Neural Network in CPU
)

# Load the specified packages
invisible(lapply(PACKAGES, library, character.only = TRUE))


```

## Managing the data to use

```{r Data to use, warning=FALSE}
# Load the data
#setwd("C:/Users/perso/Desktop/Acute_Lymphoid_Leukemia_Project-main/data/ML_for_ALL/Table_cpm")


ML_data <- read.csv("C:/Users/perso/Desktop/Acute_Lymphoid_Leukemia_Project-main/data/ML_for_ALL/Table_cpm/Total_cpm_log_expression_table_only_DEG.csv")
row.names(ML_data) <- ML_data$X
ML_data$X <- NULL
ML_data <- data.frame(t(ML_data))
ML_data$X <- rownames(ML_data)

Labels <- read.csv("C:/Users/perso/Desktop/Acute_Lymphoid_Leukemia_Project-main/data/Datasets/Post_manipulation/meta_for_ML.csv")
row.names(Labels) <- Labels$X

merged_df <- merge(ML_data, Labels, by = "X")
rownames(merged_df) <- merged_df$X
merged_df$X <- NULL
merged_df$X.pam2.clustering. <- NULL
merged_df$type <- NULL
write.csv(merged_df,file = "ML_TOT_DEG.csv", row.names = T)
ML_data <- merged_df

# Find the row indices in data that have 'Unknown' labels
unknown_indices <- which(ML_data$Cell_type == 'Unknown')

rm(merged_df,Labels)
```

## Number coating

```{r Current otimal way  }
# Number coating the values, specify the columns to be label encoded
columns_to_encode <- c("Cell_type")

# Create a new data frame without rows where 'Cell_type' has value "Unknown"
my_data_train <- subset(ML_data, Cell_type != "Unknown")
Unkown_data <- subset(ML_data, Cell_type == "Unknown")

# Convert specified columns to factor type
my_data_train <- my_data_train %>% mutate_at(columns_to_encode, as.factor)
Unkown_data <- Unkown_data %>% mutate_at(columns_to_encode, as.factor)
## as numbers
train_data_numeric <- my_data_train %>% mutate_at(columns_to_encode, as.numeric)

# Create a dictionary-like structure to store the labels
## The order corresponds to the number
my_levels <- list(Cell_type = levels(my_data_train$Cell_type)
)

rm(columns_to_encode)
```


## LASSO (Least Absolute Shrinkage and Selection Operator)


```{r Create parallel cluster}
cl <- parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)
```

```{r Corr plot to decide alpha, warning=FALSE}
set.seed(1234)
# Split the matrix
#COR <- data.matrix(ML_data[, 1:(ncol(ML_data) - 4)])
COR <- data.matrix(ML_data[, -ncol(ML_data)])
# Get the number of columns in the original matrix
num_columns <- ncol(COR)

# Randomly select 1000 column indices
selected_columns <- sample(1:num_columns, 40, replace = FALSE)
selected_columns2 <- sample(1:num_columns, 40, replace = FALSE)
selected_columns3 <- sample(1:num_columns, 40, replace = FALSE)

# Create a new matrix with only the selected columns
selected_matrix <- COR[, selected_columns]
selected_matrix2 <- COR[, selected_columns2]
selected_matrix3 <- COR[, selected_columns3]

# Set the shrinkage parameter (lambda)
#lambda <- 0.1
# Use shrinkage estimation
#Sigma_shrinkage <- cor.shrink(selected_matrix, lambda)
#Sigma_shrinkage <- data.matrix(Sigma_shrinkage)

# Use HiClimR and OpenBLAS to avoid approximations
xcor1 <- fastCor(selected_matrix, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)
xcor2 <- fastCor(selected_matrix2, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)
xcor3 <- fastCor(selected_matrix3, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)

xcorTOT <- fastCor(COR, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)

# Plot correlation matrices
corrplot(xcor1, title = "First sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90,tl.cex = 0.5)
corrplot(xcor2, title = "Second sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90,tl.cex = 0.5)
corrplot(xcor3, title = "Third sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90,tl.cex = 0.5)

corrplot(xcorTOT, title = "FULL sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90,tl.cex = 0.1)

# Remove unnecessary objects
rm(COR, num_columns, selected_columns, selected_columns2, selected_columns3,
   selected_matrix, selected_matrix2, selected_matrix3, xcor1, xcor2, xcor3,xcorTOT)

```

```{r Tuning LASSO}
set.seed(1234)
Lasso_data <- train_data_numeric
Lasso_data_f <- ML_data

# Extract data without labels
X <- as.matrix(Lasso_data[, -which(colnames(Lasso_data) == "Cell_type")])

# Fit cross-validated Lasso model
cv_lasso <- cv.glmnet(x = X,
                      y = Lasso_data$Cell_type, 
                      alpha = 0.5, 
                      grouped = FALSE,
                      parallel = TRUE,
                      relax = TRUE,
                      type.measure ="mse",
                      family = "gaussian",
                      type.gaussian = "naive",
                      nfolds = 15)
plot(cv_lasso)
abline(v = log(cv_lasso$lambda.min), col = "red", lty = 2)
abline(v = log(cv_lasso$lambda.1se), col = "green", lty = 2)

optimal_lambda <- cv_lasso$relaxed$lambda.min
```


```{r Tuning LASSO 2}
generate_lambda_sequence <- function(best_lambda, scale_factor = 0.1, num_lambdas = 5) {
  step <- best_lambda * scale_factor
  lambda_seq <- seq(best_lambda - (num_lambdas - 1) / 2 * step,
                       best_lambda + (num_lambdas - 1) / 2 * step,
                       by = step)
    return(lambda_seq)
}

lambda_seq <- generate_lambda_sequence(cv_lasso$relaxed$lambda.min, 0.1, 5)  # Generate 7 lambda values

cv_lasso <- cv.glmnet(x = X,
                      y = Lasso_data$Cell_type, 
                      alpha = 0.5, 
                      grouped = FALSE,
                      parallel = TRUE,
                      relax = TRUE,
                      type.measure ="mse",
                      family = "gaussian",
                      type.gaussian = "naive",
                      gamma = lambda_seq,
                      nfolds = 15)

plot(cv_lasso)
abline(v = log(cv_lasso$lambda.min), col = "red", lty = 2)
abline(v = log(cv_lasso$lambda.1se), col = "green", lty = 2)

optimal_lambda <- cv_lasso$relaxed$lambda.min

```

```{r Fit LASSO}
set.seed(1234)
# Extract predictors and response
X <- as.matrix(subset(Lasso_data, select = -ncol(Lasso_data)))  # Exclude the response variable
y <- as.matrix(Lasso_data$Cell_type)

# Fit a lasso regression model
lasso_model <- glmnet(X, y, 
                      alpha = 0.5, 
                      lambda = optimal_lambda,
                      family = "gaussian",
                      parallel = TRUE,
                      type.measure = "mse",
                      type.gaussian = "naive",
                      relax = TRUE)
                      

# Display selected features
plot(coef(lasso_model, s = optimal_lambda))

to_filter <- names(lasso_model$beta[, 1][lasso_model$beta[, 1] != 0])
names_imp <- data.frame(lasso_model$beta[, 1][lasso_model$beta[, 1] != 0])
# Extract non-zero coefficients from the Lasso model
selected_features <- coef(lasso_model, s = optimal_lambda, exact = TRUE, x = X, y = y)

# Filter the original dataset based on selected features

my_data_train <- subset(Lasso_data_f, select = to_filter)
#create full copy for later
#ML_data_ecoded <- ML_data 

Cell_type <- Lasso_data_f$Cell_type
my_data_train <- cbind(my_data_train, Cell_type)
Unkown_data <- subset(my_data_train, Cell_type == "Unkown")
my_data_train <- subset(my_data_train, Cell_type != "Unkown")


#setwd("D:/VarieTHOM/University/QCB/3_SEMESTRE/Data Mining/Laboratory (Blanzieri)/0_PROJECT/Datasets_finals/ML_nonHS")
#write.csv(my_data_train, file = 'CPM_nonHS_LASSO_relax.csv', row.names = TRUE)

# Remove unnecessary objects
rm(X, y, selected_features,optimal_lambda,names_imp,Lasso_data,Lasso_data_f,lasso_model,cv_lasso)

```

# Split the data

```{r Split the data}
# Split the data
# Set the seed for reproducibility
set.seed(1234)
columns_to_encode <- c("Cell_type")

# Convert specified columns to factor type
my_data_train <- my_data_train %>% mutate_at(columns_to_encode, as.factor)
#ML_data_ecoded <- ML_data %>% mutate_at(columns_to_encode, as.factor)
# Do the number coating as numbers
train_data_numeric <- my_data_train %>% mutate_at(columns_to_encode, as.numeric)

# Create an index for splitting the data
index <- createDataPartition(my_data_train$Cell_type, p = 0.7, list = FALSE)

# Create the training set
train <- my_data_train[index, ]

# Create the testing set
test <- my_data_train[-index, ]

# Create the numeric training set
train_n <- train_data_numeric[index, ]

# Create the numeric testing set
test_n <- train_data_numeric[-index, ]

rm(index)

```

```{r}

```

# Random Forest:

```{r Fin dthe best mtry}

tuned_parameters <- tuneRF(x = my_data_train[,-ncol(my_data_train)],
                           y = my_data_train$Cell_type,
                           ntreeTry = 200,
                           #mtryStart = 79,
                           stepFactor = 1.5, 
                           improve = 0.05, 
                           trace = TRUE, 
                           plot = TRUE)

rm(tuned_parameters)
```


```{r RF with cross validation from caret}

# Fit the Random Forest model
set.seed(1232)

# Define the control parameters for cross-validation
ctrl <- trainControl(method = "adaptive_cv", 
                     number = 10,
                     adaptive = list(min = 5, alpha = 0.05,method = "gls", complete = F), #gls
                     search = "random",
                     sampling = "down")


# Train the Random Forest model using cross-validation
tuneGrid <- expand.grid(mtry = 117:119, coefReg = c(0.75,0.85,0.95))
#tuneGrid <- expand.grid(mtry = 3, coefReg = 0.8)
# Train the Random Forest model using cross-validation
model_RF <- train(Cell_type ~ .,   
                  data = my_data_train,
                  method = "RRFglobal",
                  trControl = ctrl,
                  num.trees = 200,
                  importance = TRUE,
                  tuneGrid = tuneGrid)

# Display fitted models
print(model_RF)   # Using print for a more informative display
plot(model_RF, main = "Regularized Random Forest (RRF)")

# Make predictions on the test set
predictions <- predict(model_RF, newdata = test[,-ncol(test)])

cm <- confusionMatrix(data = predictions, reference = test$Cell_type, mode = "everything")
cm
# Make predictions on the new data
Predict_RF <- data.frame(predict(model_RF, newdata = Unkown_data))
Predict_RF$Cell_type <- Predict_RF$predict.model_RF..newdata...Unkown_data.
Predict_RF$predict.model_RF..newdata...Unkown_data. <- NULL
rownames(Predict_RF) <- rownames(Unkown_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(model_RF)
plot(var_imp, main = "Variable Importance Plot",top = 10)
RF_imp <- var_imp[["importance"]]


table(Predict_RF)

# Remove unnecessary objects
rm(model_RF,ctrl,var_imp,cm)
```

## K-Nearest Neighbors (KNN) Model:

```{r KKNN with CV}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "cv", 
                     number = 10,
                     adaptive = list(min = 5, alpha = 0.05,method = "gls", complete = F),
                     search = "random",
                     sampling = "down",
                     allowParallel = T)


tuneGrid <- expand.grid(kmax = 25:27, distance = c(0.55,0.65,0.75), kernel = c("optimal", "rectangular"))
#tuneGrid <- expand.grid(kmax = 17, distance = 0.6, kernel = c("optimal"))
KNN_model <- train(Cell_type ~ ., 
                   data = train, 
                   method = "kknn",
                   importance = TRUE,
                   trControl = ctrl,
                   tuneGrid = tuneGrid)

# Print the best parameters
print(KNN_model)
plot(KNN_model)
# Make predictions on the test set
predictions <- predict(KNN_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
cm = confusionMatrix(data = predictions, reference = test$Cell_type, mode = "everything")
cm


Predict_KNN <- data.frame(predict(KNN_model, newdata = Unkown_data))
Predict_KNN$disease_state <- Predict_KNN$predict.k..newdata...ML_data_ecoded.
Predict_KNN$predict.k..newdata...ML_data_ecoded. <- NULL
rownames(Predict_KNN) <- rownames(Unkown_data)

# Extract variable importance from the trained KNN model

var_imp <- varImp(KNN_model)
plot(var_imp, main = "Variable Importance Plot",top = 25)
KNN_imp <- var_imp[["importance"]]

table(Predict_KNN)

rm(ctrl, tuneGrid, predictions,k,top_20,var_imp)

```

# XGBoost (Extreme Gradient Boosting):

-Type: Ensemble Learning (boosting method) -Description: XGBoost is a powerful and efficient implementation of gradient boosting. It sequentially adds weak learners (usually decision trees) to the model, each correcting errors of the previous one. -Working: It optimizes a loss function and includes regularization terms to avoid overfitting. The final prediction is a weighted sum of the predictions from all the weak learners.


```{r Use xgb with CV from Caret}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "cv", 
                     number = 10,
                     adaptive = list(min = 5, alpha = 0.05,method = "gls", complete = TRUE),
                     search = "random",
                     sampling = "down")


# Set up the tuning grid with desired ranges for 'smooth' and 'prior'
tuneGrid <- expand.grid(nrounds = 500,lambda = c(0.1,0.2),alpha = c(0.1,0.2),eta = c(0.1,0.15,0.2))


xgb_model <- train(Cell_type ~ ., 
                   data = train, 
                   method = "xgbLinear",
                   trControl = ctrl,
                   tuneGrid = tuneGrid)

# Print the best parameters
print(xgb_model)
plot(xgb_model)

# Make predictions on the test set
predictions <- predict(xgb_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
cm = confusionMatrix(data = predictions, reference = test$Cell_type, mode = "everything")
cm
#conf_matrix <- table(predictions, test$molecular.group.ch1)
#print(conf_matrix)

Predict_XGB <- data.frame(predict(xgb_model, newdata = Unkown_data))
Predict_XGB$disease_state <- Predict_XGB$predict.xgb_model..newdata...Unkown_data.
Predict_XGB$predict.xgb_model..newdata...Unkown_data. <- NULL
rownames(Predict_XGB) <- rownames(Unkown_data)

# Extract variable importance from the trained XGB model
var_imp <- varImp(xgb_model)
plot(var_imp, main = "Importance for xgb2",top = 20)
XGB_imp <- var_imp[["importance"]]

table(Predict_XGB)

rm(ctrl, tuneGrid, predictions, xgb_model)
```

# Neural Network
```{r}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "cv", 
                     number = 10,
                     #adaptive = list(min = 5, alpha = 0.05, method = "gls", complete = TRUE),
                     #search = "random",
                     #sampling = "down",
                     allowParallel = TRUE)

# Specify the tuning parameter grid for the neural network (nnet)
tuneGrid <- expand.grid(size = 6:9, decay = c(0.25, 0.3,0.35))

# Train the neural network model
NN_model <- train(Cell_type ~ ., 
                  data = train,  
                  method = "nnet", #nnet #mlpWeightDecayML #mlpWeightDecay
                  linout = FALSE,
                  trControl = ctrl,
                  tuneGrid = tuneGrid,
                  maxit = 500
                  )



print(NN_model)
plot(NN_model)

# Make predictions on the test set
predictions <- predict(NN_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
cm = confusionMatrix(data = predictions, reference = test$Cell_type, mode = "everything")
cm

# Predict the class labels for New_data
Predict_NN <- data.frame(predict(NN_model, newdata = Unkown_data))
Predict_NN$Cell_type <- Predict_NN$predict.NN_model..newdata...Unkown_data.
Predict_NN$predict.NN_model..newdata...Unkown_data. <- NULL
rownames(Predict_NN) <- rownames(Unkown_data)

table(Predict_NN)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(NN_model)
NN_imp <- var_imp[["importance"]]

top_20 <- data.frame(subset(NN_imp, select = Overall))
top_20 <- top_20 %>% arrange(desc(Overall))
rownames(top_20) <- rownames(NN_imp)
top_20 <- data.frame(head(top_20,25))

ggplot(top_20, aes(x = Overall, y = reorder(row.names(top_20), Overall))) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 25 Values overall", x = "Importance", y = "ID") +
  theme_minimal()

# Remove unnecessary objects
rm(ctrl, tuneGrid,var_imp,NN_model,top_20)
```
# Stop cluster
```{r Stop parallel}
parallel::stopCluster(cl)
rm(cl)
```